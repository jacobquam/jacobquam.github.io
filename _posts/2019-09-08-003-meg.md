---
title: Cutting down of info using some cool tools
tags: appsec
article_header: 
  type: cover
  image:
    src: 
---
I have been playing around with some really awesome tools in Golang. Check out https://github.com/tomnomnom

I am going to cover some recon tools and workflows that I use. To start off,
I grab some general info that I use to parse further. 

General Cutting Down:

```
assetfinder --subs-only somedomain.com | tee -a domains
```
This will grab the subdomains of the specified domain name and output them to
the screen along with into a file called domains.

I now want to check for https info for all the subdomains that I have
collected:

```
cat domains| httprobe -c 50 | tee -a hosts
```

This cats all the domains previously collected, pipes it through httprobe with 
a concurrency of 50 (If you are unfamiliar with concurrency, check out the
golang specs. It's really cool, I greatly encourage it:
https://golang.org/doc/) and into a file called hosts.

Next I am going to use meg (https://github.com/tomnomnom/meg) to fetch as many
paths of each host as I can (check out TomNomNom on youtube). Meg will
automatically create a new directory called out. This directory will contain
all the referenced endpoints with request and response information. 

```
meg -v /
```

Now we can very quickly start to search through all this data looking for low
hanging fruit and information leaks, amongst other things. 

```
grep -Hnri aws_

grep -Hnri secret
``` 

just as a few examples. 

I can now use a tool called html-tool (https://github/tomnomnom/hacks/html-tool) 
to extract html information from each of the files that I have in the out
directory. The example bellow will find and parse out the attributes and src
tags located in each html document. 

```
find . -type f | html-tool attribs src
```
I am not going to go in-depth with explaining vimprev because it is a little
out of scope of this post, but I will show you how I use it. 

```
vimprev $(find . -type f)
``` 
This command will basically load all of the files in out, into vim with a easy
tabbing mode. This allows me to quickly search through all the html output by
just looking at each page. 

Now I want to use some fancy grep to get some better data to parse through:

```
grep -lri '200 ok' | grep -v ^index | xargs -n1 ls -la | sort -k5,5 -n
```

At this point, I can start to get really specific with finding interesting
stuff in these files by using a tool called urinteresting. This tool has hard
coded "interesting" aspects to look for in each file. 

```
cat domains | urintersting
```
Some fun things to look for are:

eyj (thanks https://liveoverflow.com)

The reason behind eyj is if you take a json string like: 

```
‘{"foo": 1123’ | base64 <- piped into base 64 encoded
```

you get: eyj<some other stuff> which is most likely base64 encoded json, with key
or something. 

The final next thing I might do before looking for fuzzing points is check some
wayback urls using a tool called waybackurls. Pretty self explanatory. 

```
cat domains | waybackurls > timed_urls
```
This command will cat all the domains into waybackurls and output it to a file
called timed_urls, which is the shorthand I use to specific URL's that may be
older. 

At this point, I have lots of good stuff to look through and start constructing
my list of fuzzing points and anything else I find interesting. Let the manual
testing begin. 

I also want to give a shout out to @TomNomNom and @LiveOverflow on Twitter. 
<!--more-->
